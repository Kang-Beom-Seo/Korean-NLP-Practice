{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥 러닝의 학습 방법.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXhrnuC2afY+r8jHxmse/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kang-Beom-Seo/Korean-NLP-Practice/blob/main/%EB%94%A5_%EB%9F%AC%EB%8B%9D%EC%9D%98_%ED%95%99%EC%8A%B5_%EB%B0%A9%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66-rOlX9_5de"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=len(train_X)) #배치 경사 하강법\r\n",
        "model.fit(X_train, y_train, batch_size=1) #확률적 경사 하강법(SGD)\r\n",
        "model.fit(X_train, y_train, batch_size=32) #32를 배치 크기로 하였을 경우, 미니 배치 경사 하강법\r\n",
        "keras.optimizers.SGD(lr = 0.01, momentum= 0.9) #모멘텀(Momentum), 경사 하강법에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영합니다.\r\n",
        "keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6) #아다그라드(Adagrad), 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정시킵니다.\r\n",
        "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06) #알엠에스프롭(RMSprop), 아다그라드에서의 수식과 다른 수식으로 적용. 이유는 아다그라드를 계속진행시 학습률이 거의 없어져서이다.\r\n",
        "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #아담(Adam), 알엠에스프롭과 모멘텀 두가지를 합친듯한 방법. 자주사용!!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
