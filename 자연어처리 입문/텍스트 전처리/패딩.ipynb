{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "패딩.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPf4LV1zukMYCaPrgDRS7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kang-Beom-Seo/Korean-NLP-Practice/blob/main/%ED%8C%A8%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dtPFn72W4Ef"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0D6ksIZG6FJ"
      },
      "source": [
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lfc5gDRG_OC"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm7mLPf2HI3g"
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UByEImfGHKx3"
      },
      "source": [
        "max_len = max(len(item) for item in encoded) #문장들 중에서 가장 긴 문장의 길이\r\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pme1z4nBHVT4"
      },
      "source": [
        "for item in encoded: # 각 문장에 대해서\r\n",
        "    while len(item) < max_len:   # max_len보다 작으면\r\n",
        "        item.append(0) #문장의 뒤에 0을 패딩하는 방식이다.\r\n",
        "\r\n",
        "padded_np = np.array(encoded)\r\n",
        "padded_np #zero-padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRtjYY2YHe4f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqRjIivCHwVS"
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExMQAIYQHzp-"
      },
      "source": [
        "padded = pad_sequences(encoded) #padding 방식의 default는 'pre'방식으로, 문장의 앞에서부터 패딩을 한다.\r\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLWvQHQ1H7Kq"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post') #'post'는 뒤에서부터 채운다.\r\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSkAtbbnIW1h"
      },
      "source": [
        "(padded == padded_np).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJp-JOHQIgUD"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post',maxlen = 5) #maxlen으로 길이를 제한해준다. 길이를 넘어서는 문장은 데이터가 소실된다.\r\n",
        "                                                             #확인결과, 데이터가 소실될시 padding방식에 상관없이 앞에서부터 소실된다.\r\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "216ukq8hI4f2"
      },
      "source": [
        "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\r\n",
        "print(last_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIwo7KELJjZN"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', value = last_value) #value안에는 padding에 사용될 숫자가 들어가며, 꼭 0이 아니어도 된다.\r\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUoG-ZyFJlO_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
