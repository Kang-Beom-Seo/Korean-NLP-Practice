{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "불용어.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTQifUGtuq1Vb6T2rimMHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kang-Beom-Seo/Korean-NLP-Practice/blob/main/%EB%B6%88%EC%9A%A9%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWbpA4tT2Pf"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.corpus import stopwords  #stopword는 불용어를 의미하며, 데이터에서 큰 의미가 없어 제거해야 하는 토큰들을 의미한다.\r\n",
        "stopwords.words('english')[:10]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cxmTozT-sm"
      },
      "source": [
        "print(len(stopwords.words('english')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo9gz0DHUaKm"
      },
      "source": [
        "from nltk.tokenize import word_tokenize \r\n",
        "\r\n",
        "example = \"Family is not an important thing. It's everything.\"\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "\r\n",
        "word_tokens = word_tokenize(example)\r\n",
        "\r\n",
        "result = []\r\n",
        "for w in word_tokens: #stop_word를 제외한 단어들만 추가시키는 작업\r\n",
        "    if w not in stop_words: \r\n",
        "        result.append(w) \r\n",
        "\r\n",
        "print(word_tokens) \r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhW8Hkj1UhbK"
      },
      "source": [
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\r\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\r\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\r\n",
        "# 그리고 nltk의 stopwords에는 korean은 없다... 그래서 토큰화 후에, 조사나 접속사, 기타 사용자가 원하는 불용어를 직접 만들어서 제거한다.\r\n",
        "stop_words=stop_words.split(' ')\r\n",
        "word_tokens = word_tokenize(example)\r\n",
        "\r\n",
        "result = [] \r\n",
        "for w in word_tokens: \r\n",
        "    if w not in stop_words: \r\n",
        "        result.append(w) \r\n",
        "# 위의 4줄은 아래의 한 줄로 대체 가능\r\n",
        "# result=[word for word in word_tokens if not word in stop_words]\r\n",
        "\r\n",
        "print(word_tokens) \r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9-oglTDVpRB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
