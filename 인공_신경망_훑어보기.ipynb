{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "인공 신경망 훑어보기.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3TWggRurhDHCuqZkbkxVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kang-Beom-Seo/Korean-NLP-Practice/blob/main/%EC%9D%B8%EA%B3%B5_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSXVUo5q8tSd"
      },
      "source": [
        "import numpy as np # 넘파이 사용\r\n",
        "import matplotlib.pyplot as plt # 맷플롯립 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY8sZ9tM8t1U"
      },
      "source": [
        "def step(x): #계단함수, 이제는 거의 안쓰임\r\n",
        "    return np.array(x > 0, dtype=np.int)\r\n",
        "x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\r\n",
        "y = step(x)\r\n",
        "plt.title('Step Function')\r\n",
        "plt.plot(x,y)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdG8Zal984In"
      },
      "source": [
        "# 시그모이드 함수 그래프를 그리는 코드\r\n",
        "def sigmoid(x):\r\n",
        "    return 1/(1+np.exp(-x))\r\n",
        "x = np.arange(-5.0, 5.0, 0.1)\r\n",
        "y = sigmoid(x)\r\n",
        "\r\n",
        "plt.plot(x, y)\r\n",
        "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\r\n",
        "plt.title('Sigmoid Function')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3cSkIR59N6N"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\r\n",
        "y = np.tanh(x) #하이퍼볼릭탄젠트 함수\r\n",
        "\r\n",
        "plt.plot(x, y)\r\n",
        "plt.plot([0,0],[1.0,-1.0], ':')\r\n",
        "plt.axhline(y=0, color='orange', linestyle='--')\r\n",
        "plt.title('Tanh Function')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py9RqGJc9VVj"
      },
      "source": [
        "def relu(x): #시그모이드보다 많이쓰임, 입력값이 음수면 dying ReLU문제가 있음\r\n",
        "    return np.maximum(0, x)\r\n",
        "\r\n",
        "x = np.arange(-5.0, 5.0, 0.1)\r\n",
        "y = relu(x)\r\n",
        "\r\n",
        "plt.plot(x, y)\r\n",
        "plt.plot([0,0],[5.0,0.0], ':')\r\n",
        "plt.title('Relu Function')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TWj0z919lVD"
      },
      "source": [
        "a = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb6nlN8B9x4t"
      },
      "source": [
        "def leaky_relu(x): #ReLU를 보완한 Leaky_ReLU\r\n",
        "    return np.maximum(a*x, x)\r\n",
        "\r\n",
        "x = np.arange(-5.0, 5.0, 0.1)\r\n",
        "y = leaky_relu(x)\r\n",
        "\r\n",
        "plt.plot(x, y)\r\n",
        "plt.plot([0,0],[5.0,0.0], ':')\r\n",
        "plt.title('Leaky ReLU Function')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwacUjpI9zWW"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성\r\n",
        "y = np.exp(x) / np.sum(np.exp(x)) #소프트맥스 함수\r\n",
        "\r\n",
        "plt.plot(x, y)\r\n",
        "plt.title('Softmax Function')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhv2bUTP-JHp"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "model = Sequential() # 층을 추가할 준비\r\n",
        "model.add(Dense(8, input_dim=4, kernel_initializer='uniform', activation='relu'))\r\n",
        "# 입력층(4)과 다음 은닉층(8) 그리고 은닉층의 활성화 함수는 relu\r\n",
        "model.add(Dense(8, activation='relu')) # 은닉층(8)의 활성화 함수는 relu\r\n",
        "model.add(Dense(3, activation='softmax')) # 출력층(3)의 활성화 함수는 softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2dL69hm-NiZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}