{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nimport numpy as np\\nimport pickle\\ntf.set_random_seed(10)\\n\\n# 초성 리스트. 00 ~ 18 --> 19개\\nCHOSUNG_LIST = [\\'ᄀ\\', \\'ᄁ\\', \\'ᄂ\\', \\'ᄃ\\', \\'ᄄ\\', \\'ᄅ\\', \\'ᄆ\\', \\'ᄇ\\', \\'ᄈ\\', \\'ᄉ\\', \\'ᄊ\\', \\'ᄋ\\', \\'ᄌ\\', \\'ᄍ\\', \\'ᄎ\\', \\'ᄏ\\', \\'ᄐ\\', \\'ᄑ\\', \\'ᄒ\\']\\n\\n# 중성 리스트. 00 ~ 20 --> 21개\\nJUNGSUNG_LIST = [\\'ᅡ\\', \\'ᅢ\\', \\'ᅣ\\', \\'ᅤ\\', \\'ᅥ\\', \\'ᅦ\\', \\'ᅧ\\', \\'ᅨ\\', \\'ᅩ\\', \\'ᅪ\\', \\'ᅫ\\', \\'ᅬ\\', \\'ᅭ\\', \\'ᅮ\\', \\'ᅯ\\', \\'ᅰ\\', \\'ᅱ\\', \\'ᅲ\\', \\'ᅳ\\', \\'ᅴ\\',\\n                 \\'ᅵ\\']\\n\\n# 종성 리스트. 00 ~ 27 + 1(1개는 종성없음코드) --> 28개\\nJONGSUNG_LIST = [\\' \\', \\'ᆨ\\', \\'ᆩ\\', \\'ᆪ\\', \\'ᆫ\\', \\'ᆬ\\', \\'ᆭ\\', \\'ᆮ\\', \\'ᆯ\\', \\'ᆰ\\', \\'ᆱ\\', \\'ᆲ\\', \\'ᆳ\\', \\'ᆴ\\', \\'ᆵ\\', \\'ᆶ\\', \\'ᆷ\\', \\'ᆸ\\', \\'ᆹ\\', \\'ᆺ\\',\\n                 \\'ᆻ\\', \\'ᆼ\\', \\'ᆽ\\', \\'ᆾ\\', \\'ᆿ\\', \\'ᇀ\\', \\'ᇁ\\', \\'ᇂ\\']\\n\\n# 독립 자소 리스트. --> 51개\\nINDI_LIST = [\\'ㄱ\\', \\'ㄲ\\', \\'ㄳ\\', \\'ㄴ\\', \\'ㄵ\\', \\'ㄶ\\', \\'ㄷ\\', \\'ㄸ\\', \\'ㄹ\\', \\'ㄺ\\', \\'ㄻ\\', \\'ㄼ\\', \\'ㄽ\\', \\'ㄾ\\', \\'ㄿ\\', \\'ㅀ\\', \\'ㅁ\\', \\'ㅂ\\', \\'ㅃ\\', \\'ㅄ\\', \\'ㅅ\\',\\n             \\'ㅆ\\', \\'ㅇ\\', \\'ㅈ\\', \\'ㅉ\\', \\'ㅊ\\', \\'ㅋ\\', \\'ㅌ\\', \\'ㅍ\\', \\'ㅎ\\', \\'ㅏ\\', \\'ㅐ\\', \\'ㅑ\\', \\'ㅒ\\', \\'ㅓ\\', \\'ㅔ\\', \\'ㅕ\\', \\'ㅖ\\', \\'ㅗ\\', \\'ㅘ\\', \\'ㅙ\\', \\'ㅚ\\',\\n             \\'ㅛ\\', \\'ㅜ\\', \\'ㅝ\\', \\'ㅞ\\', \\'ㅟ\\', \\'ㅠ\\', \\'ㅡ\\', \\'ㅢ\\', \\'ㅣ\\']\\n\\ndef syllable(char):\\n    s = ord(char) - 44032\\n    cho = (s//21)//28\\n    jung = (s%(21*28))//28\\n    jong = (s%28)\\n    \\n    return CHOSUNG_LIST[cho], JUNGSUNG_LIST[jung], JONGSUNG_LIST[jong]\\n\\n# 자소의 차원은 119\\nJASO_DIM = len(CHOSUNG_LIST)+len(JUNGSUNG_LIST)+len(JONGSUNG_LIST)+len(INDI_LIST)\\n\\nhangul_johab = range(44032,55204)\\nhangul_jaeum = range(12593,12623)\\nhangul_moeum = range(12623,12644)\\nhangul_chosung = range(4352,4371)\\nhangul_jungsung = range(4449,4470)\\nhangul_jongsung = range(4520,4547)\\nenglish1 = range(65,91)\\nenglish2 = range(97,123)\\ndigit = range(48,58)\\nspecial_char = [ord(\\'.\\'), ord(\\'\\'\\'), ord(\\'?\\'), ord(\\',\\'), ord(\\'!\\'), ord(\\'%\\')] # 형태소 분석에 필요하다고 생각하는 특수문자 추가\\n\\ndef read_data(file_path):\\n    sentence = []\\n    data = []\\n    label = []\\n    d_append = data.append\\n    with open(file_path,\"r\") as f:\\n        for line in f.readlines():\\n            if line != \\'\\n\\':\\n                w = line.split(\\'\\t\\')\\n                label.append(w[1].replace(\\'\\n\\',\\'\\'))\\n                word = []\\n                w_append = word.append\\n                w_extend = word.extend\\n                for c in w[0]:\\n                    sign_unk = 0\\n                    \\n                    if ord(c) in hangul_johab or ord(c) in hangul_chosung or                        ord(c) in hangul_jungsung or ord(c) in hangul_jongsung or                        ord(c) in hangul_jaeum or ord(c) in hangul_moeum or                        ord(c) in english1 or ord(c) in english2 or                        ord(c) in digit or ord(c) in special_char: pass\\n                    else: sign_unk = 1 # 지정된 한글, 영어, 숫자, 특수문자 이외에 전부 UNK태그 지정\\n\\n                    if sign_unk == 1:\\n                        w_append(\\'<UNK>\\')\\n                    else:\\n                        if ord(c) in hangul_johab: # 조합형 한글은 자모를 분리\\n                            jaso = syllable(c)\\n                            w_extend(jaso)\\n                        else:\\n                            w_append(c) # 한글자모, 영어, 숫자는 그대로\\n                sentence.append((word,w[1].replace(\\'\\n\\',\\'\\'))) # ([분리된 형태소],태그) 형태로 저장\\n            else:\\n                if sentence != []:\\n                    d_append(sentence) # sentence마다 구분지어서 저장\\n                    sentence = []\\n    return data,label\\nprint(\"\\n==Read Data==\\n\")\\ndata, label = read_data(\"data_v5_edit.txt\") # data는 3차원 리스트로 \\n                                                       # 전체 데이터 -> 문장 -> 형태소 순으로 저장됨\\n\\npickle.dump(data, open(\\'data.pkl\\',\\'wb\\'))\\npickle.dump(label, open(\\'label.pkl\\',\\'wb\\'))\\n\\nsentence_max_length = 0\\nword_max_length = 0\\nfor sen in data:\\n    if sentence_max_length < len(sen): sentence_max_length = len(sen)\\n    for word in sen:\\n        if word_max_length < len(word[0]): word_max_length = len(word[0])\\n\\nchar_list = [\\'<UNK>\\']+CHOSUNG_LIST+JUNGSUNG_LIST+JONGSUNG_LIST+INDI_LIST+[chr(i) for i in english1]               + [chr(i) for i in english2] + [chr(i) for i in digit] + [chr(i) for i in special_char]\\ndictionary_char = dict()\\n\\nfor i in char_list:\\n    dictionary_char[i] = len(dictionary_char)\\n\\nlabel = sorted(label)\\n\\ndictionary_label= dict()\\nfor i in label:\\n    dictionary_label[i] = len(dictionary_label)\\n\\ndictionary_char\\n\\n# 위에서 정의된 dictionary에 따라 데이터를 index로 치환\\ndef make_dataSet(data, dictionary_char):\\n    indexed_data = [] \\n    d_append = indexed_data.append\\n    for sentence in data:\\n        sen = []\\n        lab = []\\n        s_append = sen.append\\n        l_append = lab.append\\n        for word in sentence:\\n            s_append(([dictionary_char[char] for char in word[0]], dictionary_label[word[1]]))\\n        d_append(sen)\\n    \\n    return indexed_data\\n\\nindexed_data = make_dataSet(data,dictionary_char)\\n\\ndictionary_label'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "tf.set_random_seed(10)\n",
    "\n",
    "# 초성 리스트. 00 ~ 18 --> 19개\n",
    "CHOSUNG_LIST = ['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ']\n",
    "\n",
    "# 중성 리스트. 00 ~ 20 --> 21개\n",
    "JUNGSUNG_LIST = ['ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ',\n",
    "                 'ᅵ']\n",
    "\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개는 종성없음코드) --> 28개\n",
    "JONGSUNG_LIST = [' ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ',\n",
    "                 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
    "\n",
    "# 독립 자소 리스트. --> 51개\n",
    "INDI_LIST = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ',\n",
    "             'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ',\n",
    "             'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "def syllable(char):\n",
    "    s = ord(char) - 44032\n",
    "    cho = (s//21)//28\n",
    "    jung = (s%(21*28))//28\n",
    "    jong = (s%28)\n",
    "    \n",
    "    return CHOSUNG_LIST[cho], JUNGSUNG_LIST[jung], JONGSUNG_LIST[jong]\n",
    "\n",
    "# 자소의 차원은 119\n",
    "JASO_DIM = len(CHOSUNG_LIST)+len(JUNGSUNG_LIST)+len(JONGSUNG_LIST)+len(INDI_LIST)\n",
    "\n",
    "hangul_johab = range(44032,55204)\n",
    "hangul_jaeum = range(12593,12623)\n",
    "hangul_moeum = range(12623,12644)\n",
    "hangul_chosung = range(4352,4371)\n",
    "hangul_jungsung = range(4449,4470)\n",
    "hangul_jongsung = range(4520,4547)\n",
    "english1 = range(65,91)\n",
    "english2 = range(97,123)\n",
    "digit = range(48,58)\n",
    "special_char = [ord('.'), ord('\\''), ord('?'), ord(','), ord('!'), ord('%')] # 형태소 분석에 필요하다고 생각하는 특수문자 추가\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentence = []\n",
    "    data = []\n",
    "    label = []\n",
    "    d_append = data.append\n",
    "    with open(file_path,\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line != '\\n':\n",
    "                w = line.split('\\t')\n",
    "                label.append(w[1].replace('\\n',''))\n",
    "                word = []\n",
    "                w_append = word.append\n",
    "                w_extend = word.extend\n",
    "                for c in w[0]:\n",
    "                    sign_unk = 0\n",
    "                    \n",
    "                    if ord(c) in hangul_johab or ord(c) in hangul_chosung or \\\n",
    "                       ord(c) in hangul_jungsung or ord(c) in hangul_jongsung or \\\n",
    "                       ord(c) in hangul_jaeum or ord(c) in hangul_moeum or \\\n",
    "                       ord(c) in english1 or ord(c) in english2 or \\\n",
    "                       ord(c) in digit or ord(c) in special_char: pass\n",
    "                    else: sign_unk = 1 # 지정된 한글, 영어, 숫자, 특수문자 이외에 전부 UNK태그 지정\n",
    "\n",
    "                    if sign_unk == 1:\n",
    "                        w_append('<UNK>')\n",
    "                    else:\n",
    "                        if ord(c) in hangul_johab: # 조합형 한글은 자모를 분리\n",
    "                            jaso = syllable(c)\n",
    "                            w_extend(jaso)\n",
    "                        else:\n",
    "                            w_append(c) # 한글자모, 영어, 숫자는 그대로\n",
    "                sentence.append((word,w[1].replace('\\n',''))) # ([분리된 형태소],태그) 형태로 저장\n",
    "            else:\n",
    "                if sentence != []:\n",
    "                    d_append(sentence) # sentence마다 구분지어서 저장\n",
    "                    sentence = []\n",
    "    return data,label\n",
    "print(\"\\n==Read Data==\\n\")\n",
    "data, label = read_data(\"data_v5_edit.txt\") # data는 3차원 리스트로 \n",
    "                                                       # 전체 데이터 -> 문장 -> 형태소 순으로 저장됨\n",
    "\n",
    "pickle.dump(data, open('data.pkl','wb'))\n",
    "pickle.dump(label, open('label.pkl','wb'))\n",
    "\n",
    "sentence_max_length = 0\n",
    "word_max_length = 0\n",
    "for sen in data:\n",
    "    if sentence_max_length < len(sen): sentence_max_length = len(sen)\n",
    "    for word in sen:\n",
    "        if word_max_length < len(word[0]): word_max_length = len(word[0])\n",
    "\n",
    "char_list = ['<UNK>']+CHOSUNG_LIST+JUNGSUNG_LIST+JONGSUNG_LIST+INDI_LIST+[chr(i) for i in english1]\\\n",
    "               + [chr(i) for i in english2] + [chr(i) for i in digit] + [chr(i) for i in special_char]\n",
    "dictionary_char = dict()\n",
    "\n",
    "for i in char_list:\n",
    "    dictionary_char[i] = len(dictionary_char)\n",
    "\n",
    "label = sorted(label)\n",
    "\n",
    "dictionary_label= dict()\n",
    "for i in label:\n",
    "    dictionary_label[i] = len(dictionary_label)\n",
    "\n",
    "dictionary_char\n",
    "\n",
    "# 위에서 정의된 dictionary에 따라 데이터를 index로 치환\n",
    "def make_dataSet(data, dictionary_char):\n",
    "    indexed_data = [] \n",
    "    d_append = indexed_data.append\n",
    "    for sentence in data:\n",
    "        sen = []\n",
    "        lab = []\n",
    "        s_append = sen.append\n",
    "        l_append = lab.append\n",
    "        for word in sentence:\n",
    "            s_append(([dictionary_char[char] for char in word[0]], dictionary_label[word[1]]))\n",
    "        d_append(sen)\n",
    "    \n",
    "    return indexed_data\n",
    "\n",
    "indexed_data = make_dataSet(data,dictionary_char)\n",
    "\n",
    "dictionary_label'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(indexed_data,open('indexed_data.pkl','wb'))\n",
    "# pickle.dump(dictionary_char,open('char_dict.pkl','wb'))\n",
    "# pickle.dump(dictionary_label,open('label_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from preprocessing import Preprocessing\n",
    "tf.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dim = 100\n",
    "word_max_length = 32\n",
    "X = tf.placeholder(tf.float32, [1, word_max_length, char_dim, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(1, 32, 100, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = [3,5,7,9]\n",
    "num_filters = 25\n",
    "conved_layers = []\n",
    "for f in filter_size:\n",
    "    Y = tf.layers.conv2d(X, filters=num_filters, kernel_size=[f, char_dim], activation=tf.nn.relu, padding= 'VALID',\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    Y = tf.layers.max_pooling2d(Y, pool_size=[word_max_length - f + 1, 1], strides=4, padding='VALID')\n",
    "    conved_layers.append(Y)\n",
    "concat_chars = tf.concat(conved_layers,3)\n",
    "#out_word = tf.squeeze(concat_chars)\n",
    "#out_word = tf.reshape(concat_chars,[-1,15,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'max_pooling2d/MaxPool:0' shape=(1, 1, 1, 25) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling2d_1/MaxPool:0' shape=(1, 1, 1, 25) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling2d_2/MaxPool:0' shape=(1, 1, 1, 25) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling2d_3/MaxPool:0' shape=(1, 1, 1, 25) dtype=float32>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conved_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(1, 1, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def make_input (indexed_data, window_size = 7, max_length = 48, char_dim = 188):\\n    for sentence in indexed_data:\\n        for w_index in range(len(sentence)):\\n            sentence_input = np.zeros([window_size*2 +1, max_length, char_dim], dtype=np.float32)\\n            word_index = 6 - w_index\\n            for word in sentence:\\n                word_index = word_index + 1\\n                if word_index >= 15 or word_index<0 : continue\\n                \\n                char_index = 0\\n                if len(word[0]) % 2 == 0:\\n                    char_index = max_length//2 - len(word[0]) // 2\\n                else:\\n                    char_index = max_length//2 - (len(word[0]) // 2 + 1)\\n                \\n                for i, char in enumerate(word[0]):\\n                    sentence_input[word_index][int(i+char_index)][char] = 1\\n            sentence_input = np.reshape(sentence_input, [window_size*2 +1, max_length, char_dim,1])\\n            yield sentence_input\\n\\nlen(indexed_data)\\n\\nwith tf.Session() as sess:\\n    tf.global_variables_initializer().run()\\n    X_input = make_input(indexed_data)\\n    y_1 = sess.run(out_word,feed_dict={X:next(X_input)})\\n\\n## 101번째 index부터 position embedding & binary feature\\n## 7번째 target vector는 binary feature가 1\\n\\ny_1[0][7]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def make_input (indexed_data, window_size = 7, max_length = 48, char_dim = 188):\n",
    "    for sentence in indexed_data:\n",
    "        for w_index in range(len(sentence)):\n",
    "            sentence_input = np.zeros([window_size*2 +1, max_length, char_dim], dtype=np.float32)\n",
    "            word_index = 6 - w_index\n",
    "            for word in sentence:\n",
    "                word_index = word_index + 1\n",
    "                if word_index >= 15 or word_index<0 : continue\n",
    "                \n",
    "                char_index = 0\n",
    "                if len(word[0]) % 2 == 0:\n",
    "                    char_index = max_length//2 - len(word[0]) // 2\n",
    "                else:\n",
    "                    char_index = max_length//2 - (len(word[0]) // 2 + 1)\n",
    "                \n",
    "                for i, char in enumerate(word[0]):\n",
    "                    sentence_input[word_index][int(i+char_index)][char] = 1\n",
    "            sentence_input = np.reshape(sentence_input, [window_size*2 +1, max_length, char_dim,1])\n",
    "            yield sentence_input\n",
    "\n",
    "len(indexed_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    X_input = make_input(indexed_data)\n",
    "    y_1 = sess.run(out_word,feed_dict={X:next(X_input)})\n",
    "\n",
    "## 101번째 index부터 position embedding & binary feature\n",
    "## 7번째 target vector는 binary feature가 1\n",
    "\n",
    "y_1[0][7]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ForClass]",
   "language": "python",
   "name": "conda-env-ForClass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
