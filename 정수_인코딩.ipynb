{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "정수 인코딩.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6XA28pUMbYxr2tGx6iIiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kang-Beom-Seo/Korean-NLP-Practice/blob/main/%EC%A0%95%EC%88%98_%EC%9D%B8%EC%BD%94%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3bzNl5d2jPl"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYNCa8f2nt_"
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuPrccdD2re3"
      },
      "source": [
        "# 문장 토큰화\r\n",
        "text = sent_tokenize(text)\r\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_37G-Us2vYV"
      },
      "source": [
        "# 정제와 단어 토큰화\r\n",
        "vocab = {} # 파이썬의 dictionary 자료형\r\n",
        "sentences = []\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "for i in text:\r\n",
        "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\r\n",
        "    result = []\r\n",
        "\r\n",
        "    for word in sentence: \r\n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다. 또한, 대문자와 소문자가 구별되면 서로 다른 단어로 인식될 수 있기 때문에, 소문자화 합니다.\r\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\r\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\r\n",
        "                result.append(word)\r\n",
        "                if word not in vocab:\r\n",
        "                    vocab[word] = 0 \r\n",
        "                vocab[word] += 1\r\n",
        "    sentences.append(result) \r\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atfXYCNS3In7"
      },
      "source": [
        "print(vocab) #단어의 빈도수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM-krXFX36M_"
      },
      "source": [
        "print(vocab[\"barber\"]) #'barber'라는 단어의 빈도수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WudDT3594BHO"
      },
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True) #key에는 정렬조건이 들어가며, x[1]이므로 (단어, 빈도수)에서 빈도수가 정렬기준이 된다.\r\n",
        "                                                                          #그리고 reverse인수는 True이면 내림차순, False는 오름차순이다. default는 False이다.\r\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDGoo5Sz4wYi"
      },
      "source": [
        "word_to_index = {}\r\n",
        "i=0\r\n",
        "for (word, frequency) in vocab_sorted :\r\n",
        "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\r\n",
        "        i=i+1\r\n",
        "        word_to_index[word] = i\r\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn9CxQS25W5W"
      },
      "source": [
        "vocab_size = 5\r\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\r\n",
        "for w in words_frequency:\r\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\r\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4wODu9j6s1g"
      },
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1 #'OOV'는 Out Of Vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhp2s3c66zbY"
      },
      "source": [
        "encoded = []\r\n",
        "for s in sentences:\r\n",
        "    temp = []\r\n",
        "    for w in s:\r\n",
        "        try:\r\n",
        "            temp.append(word_to_index[w])\r\n",
        "        except KeyError:\r\n",
        "            temp.append(word_to_index['OOV'])\r\n",
        "    encoded.append(temp)\r\n",
        "print(encoded) #각 문장들이 정수로 인코딩되었다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wJNlSO7FOt"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McgarVSW7SJ3"
      },
      "source": [
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I33mc2xt7Th3"
      },
      "source": [
        "words = sum(sentences, []) #리스트 합치기\r\n",
        "# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\r\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owGCrbYu7btS"
      },
      "source": [
        "vocab = Counter(words) # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.\r\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmR_qu69wVB"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox_vWcIu91CV"
      },
      "source": [
        "vocab_size = 5\r\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\r\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkJX1D7V-BFN"
      },
      "source": [
        "word_to_index = {}\r\n",
        "i = 0\r\n",
        "for (word, frequency) in vocab :\r\n",
        "    i = i+1\r\n",
        "    word_to_index[word] = i\r\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0PiNby1-Dua"
      },
      "source": [
        "from nltk import FreqDist\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywCMPvvV-SBr"
      },
      "source": [
        "# np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...\r\n",
        "vocab = FreqDist(np.hstack(sentences)) #가로방향으로 합치는 hstack인데, 실제로 추가되는 배열은 없으므로 sentences안에 있는 요소들만 합쳐진다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L-1dtvd_Grr"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoHu54SdAlPA"
      },
      "source": [
        "vocab_size = 5\r\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\r\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF9FHME4Aqbv"
      },
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\r\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVfPVI2jA9TP"
      },
      "source": [
        "test=['a', 'b', 'c', 'd', 'e']\r\n",
        "for index, value in enumerate(test): # 입력의 순서대로 0부터 인덱스를 부여함. 그리고 보통 for문보다 실행이 빠르다고 함.\r\n",
        "  print(\"value : {}, index: {}\".format(value, index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAO7g_YdBMcy"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUg6hzSDBVW9"
      },
      "source": [
        "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_h_0Lo0BXOD"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgtkgz-uBZKQ"
      },
      "source": [
        "print(tokenizer.word_index) #빈도수가 높은 단어 순서대로 낮은 인덱스가 부여된다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVnyZkdxBkfp"
      },
      "source": [
        "print(tokenizer.word_counts) #단어의 빈도수가 나온다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxS1wO0WB6JG"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences)) #입력으로 들어온 코퍼스를 위에서 정해진 인덱스로 변환해준다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IM9Al08CJcC"
      },
      "source": [
        "vocab_size = 5\r\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용, 여기서 +1을 한 이유는 숫자 0도 포함해서 세기 때문이다.\r\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl5t2v12C_UD"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences)) #상위 5개 단어만 사용하는 부분은 texts_to_sequences부분에서 적용된다.\r\n",
        "                                               #위의 word_counts와 word_index에서는 5개 이외의 단어들에 대해서도 나온다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDIVLcBDIo5"
      },
      "source": [
        "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\r\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPTJipVsDk37"
      },
      "source": [
        "vocab_size = 5\r\n",
        "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\r\n",
        "for w in words_frequency:\r\n",
        "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\r\n",
        "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\r\n",
        "                                #굳이 이런 방식으로 지울 필요까지는 사실 없다.\r\n",
        "print(tokenizer.word_index)\r\n",
        "print(tokenizer.word_counts)\r\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0fWRIjqDmf5"
      },
      "source": [
        "vocab_size = 5\r\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\r\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\r\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWxbnXrbD1QS"
      },
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV'])) #기본적으로 케라스 토크나이저는 'OOV' 인덱스를 1로 지정한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjdRH_KUEEjI"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY1___BxEe86"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}